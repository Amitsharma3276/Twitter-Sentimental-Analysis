{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/youben/twitter-sentiment-analysis/notebook?select=train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#This is for making some large tweets to be displyed\n",
    "pd.options.display.max_colwidth=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(r\"train.csv\",encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL friend.............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trailer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!       T_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>99996</td>\n",
       "      <td>0</td>\n",
       "      <td>@Cupcake  seems like a repeating problem   hope you're able to find something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>99997</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "      <td>@CuPcAkE_2120 ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>@Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>100000</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake_kayla haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID  Sentiment  \\\n",
       "0           1          0   \n",
       "1           2          0   \n",
       "2           3          1   \n",
       "3           4          0   \n",
       "4           5          0   \n",
       "...       ...        ...   \n",
       "99984   99996          0   \n",
       "99985   99997          1   \n",
       "99986   99998          0   \n",
       "99987   99999          1   \n",
       "99988  100000          1   \n",
       "\n",
       "                                                                                             SentimentText  \n",
       "0                                                                 is so sad for my APL friend.............  \n",
       "1                                                                         I missed the New Moon trailer...  \n",
       "2                                                                                  omg its already 7:30 :O  \n",
       "3      .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get ...  \n",
       "4                                                             i think mi bf is cheating on me!!!       T_T  \n",
       "...                                                                                                    ...  \n",
       "99984                       @Cupcake  seems like a repeating problem   hope you're able to find something.  \n",
       "99985  @cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll se...  \n",
       "99986                                                                       @CuPcAkE_2120 ya i thought so   \n",
       "99987                                        @Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me.   \n",
       "99988                                                                      @cupcake_kayla haha yes you do   \n",
       "\n",
       "[99989 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15052    (contd...) so good last night?? Along with other crappy songs that we're of a 6th class standard...\n",
       "19662                                                @_Enigma__ looking forward to your return... take care \n",
       "94041                                                @coreyellerbe  I know.  How far the Cubs have fallen.  \n",
       "78378                   @archonline time u went to the tweetups ... BTW am guilty of not attending them too \n",
       "23079    @11twenty LOL. I need some fulltime clients or a fulltime agency. if you got the hookup let me k...\n",
       "16071                                                .. have a look at the activist cow  http://bit.ly/tRlcl\n",
       "12014    (symphonysoldier.com) OMG. So shocked. I mean seriously Ian? Out of all the people in the world,...\n",
       "53547    @aubreyoday... I love that show...weeds! But i dont get it here in london  i miss my American tv...\n",
       "71866    @calamur  yeah... i guess my password was also got changed... i was not able to login my FB  cha...\n",
       "37275                       @amanda_baybeee Yea I was!  Wednesday? Is that at the ABC? Do you have tickets? \n",
       "59463                  @Bekei87 i tried that.   did you put just ur No. or the +61 in front?  i tried both  \n",
       "66739                              @blundell07 Aye, I loved it too  @cyntaxerror over analyzed it me thinks \n",
       "88452                                                                                   @chellanglo anytime \n",
       "78691           @Calena_btv Yeah, I know  It was really cool, because it felt real.. was sad when I woke up \n",
       "66413                                                @brigwyn Oh hey! Grats on your 'welcome aboard' email! \n",
       "96357                                                                 @Croc_Hunteress I meant 1000th Tweet. \n",
       "70870    @BxChic7399 ugh im mad u can change ya pics like dat n i can only get dis ugly ass pic i cropped...\n",
       "21046                                                                        @_RealGC_ hi~thx for following \n",
       "29529                                                             @adactio is this a homer simpson moment ? \n",
       "32355      @agathaxx Caffe Nero keeping u fed homie. My Dads in Dubai for em...lost my darts bud for a week \n",
       "55546                                                                                      @b50 lol! thanks \n",
       "32453    @Agent_M ah. knowing microsoft, they could probby do it, but would want to charge way too much e...\n",
       "17069                                                                        ....RIP Mr David Carradine.... \n",
       "82183    @carriebeth I was so scared. I have to hand my cv into them by friday for assessing. I hate stuf...\n",
       "35932    @alexalbrecht I'm worried about Project Lore. Don't get me wrong, Dorkins is great, but come on... \n",
       "7452                     #wiebe so close!  why does he only get one shot?!  oh well, ur still my hero wiebe \n",
       "65091                            @BJDY3R That's impossible. My dog, Buttons, is the cutest dog in the world \n",
       "66491                     @brinshannara Not enough to say I do.  I grew up in GA so most of us took Spanish.\n",
       "8818             !@dossy see, move is neat, can be done, and provides deniablity against the incumbent game \n",
       "53236    @B_NERD well i know it true  did u not forget the presells he did on he release date? hes a hate...\n",
       "29943    @AlanCarr i'm disappointed Alan i just spent ages downloading the software to listen to your sho...\n",
       "29065                                                             @AceMas21 I have twitter diahorrea tonite \n",
       "82471                                                             @chemiosmosis dont worry they are fixable \n",
       "93158                    @BawldGuy that post hit the spot. How can you beat &quot;self righteous dung&quot; \n",
       "91865                                                                       @ColiHellraiser you'll be fine! \n",
       "94971                                   @Cpunches NO! I was folding laundry and came back and you were gone \n",
       "87108    @christt I really wanted to go  i didn't get to go to any of the anal beard gigs  AGAIN!!!! suck...\n",
       "87191                                     @charleyw We just want to make sure you are who you say you are  !\n",
       "7811     #myweakness is me and my friends making fun of the clothes this certain teacher at my school wea...\n",
       "84857    @cazzwright I would value your opinion on their work. No pressure, just for fun  a particular ar...\n",
       "76240                              @antrix yeah true! until I tried Debian Lenny.  btw, are you using ext4 ?\n",
       "28487                                                    @aion_ayase gah no beta key here*ragequitsTwitter* \n",
       "33893                                               @aj1996 Thank you.  Hope I can make the chat. Gonna try!\n",
       "54097                         @aussiehost you're welcome  just 97 more people seeing your service offerings!\n",
       "27504                                                                   @agnesjames And look! No hang over! \n",
       "859                                                                      Didn't win the twitter contest. BOO\n",
       "15837                         .@PeytonSawyer Sounds good to me.... 'cept I don't know who Peyton Sawyer is. \n",
       "65177                                                    @brdwayboyts I'm jealous of your eating of Chinese \n",
       "161       ... had weird encounters with so many homeless/creeps on the walk home tonight all by myself   ...\n",
       "83233                   @cassandraleighh good luck &amp; do well on that math test  ily http://myloc.me/2sKX\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will now look take a random tweets to gain more insights\n",
    "\n",
    "rand_indexes=np.random.randint(1,len(train_data),50).tolist()\n",
    "train_data['SentimentText'][rand_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For me, after some execution, I noticed this:\n",
    "\n",
    "1. There is tweets with a url (like tweet 35546): we must think about a way to handle URLs, I thought about deleting them because a domain name or the protocol used will not make someone happy or sad unless the domain name is 'food.com'.\n",
    "2. The use of hashtags: we should keep only the words without '#' so words like python and the hashtag '#python' can be seen as the same word, and of course they are.\n",
    "3. Words like 'as', 'to' and 'so' should be deleted, because they only serve as a way to link phrases and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internet language includes so many emoticons, people also tend to create their own, so we will first analyze the emoticons included in our dataset, try to classify them as happy and said, and make sure that our model know about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3281, ':/'),\n",
       " (2874, 'x '),\n",
       " (2626, ': '),\n",
       " (1339, 'x@'),\n",
       " (1214, 'xx'),\n",
       " (1162, 'xa'),\n",
       " (984, ';3'),\n",
       " (887, 'xp'),\n",
       " (842, 'xo'),\n",
       " (713, ';)'),\n",
       " (483, 'xe'),\n",
       " (431, ';I'),\n",
       " (353, ';.'),\n",
       " (254, 'xD'),\n",
       " (251, 'x.'),\n",
       " (245, '::'),\n",
       " (234, 'X '),\n",
       " (217, ';t'),\n",
       " (209, ';s'),\n",
       " (185, ':O'),\n",
       " (176, ':3'),\n",
       " (166, ';D'),\n",
       " (159, \":'\"),\n",
       " (157, 'XD'),\n",
       " (146, 'x3'),\n",
       " (142, ':p'),\n",
       " (126, \":'(\"),\n",
       " (118, ':@'),\n",
       " (117, 'xh'),\n",
       " (117, ':S'),\n",
       " (109, 'xm'),\n",
       " (104, ';p'),\n",
       " (104, ';-)'),\n",
       " (92, ':|'),\n",
       " (91, 'x,'),\n",
       " (89, ';P'),\n",
       " (76, 'xd'),\n",
       " (75, ';o'),\n",
       " (75, ';d'),\n",
       " (71, ':o'),\n",
       " (65, 'XX'),\n",
       " (63, ':L'),\n",
       " (59, 'Xx'),\n",
       " (59, ':1'),\n",
       " (58, ':]'),\n",
       " (57, ':s'),\n",
       " (56, ':0'),\n",
       " (54, 'XO'),\n",
       " (44, ';;'),\n",
       " (43, ';('),\n",
       " (38, ':-D'),\n",
       " (37, 'xk'),\n",
       " (36, 'XT'),\n",
       " (35, 'x?'),\n",
       " (35, 'x)'),\n",
       " (34, 'x2'),\n",
       " (33, ';/'),\n",
       " (32, 'x:'),\n",
       " (32, ':\\\\'),\n",
       " (31, 'x-'),\n",
       " (27, 'Xo'),\n",
       " (27, 'XP'),\n",
       " (27, ':-/'),\n",
       " (26, ':-P'),\n",
       " (25, ':*'),\n",
       " (23, 'xX'),\n",
       " (22, \":')\"),\n",
       " (17, 'xP'),\n",
       " (16, ':['),\n",
       " (16, ':-p'),\n",
       " (14, 'x]'),\n",
       " (14, 'XM'),\n",
       " (13, ':-O'),\n",
       " (12, 'x('),\n",
       " (12, 'X1'),\n",
       " (12, ':x'),\n",
       " (11, 'XS'),\n",
       " (11, ':l'),\n",
       " (10, 'x*'),\n",
       " (10, 'X.'),\n",
       " (10, ':b'),\n",
       " (10, ':T'),\n",
       " (9, ';]'),\n",
       " (9, ':I'),\n",
       " (8, ':C'),\n",
       " (7, ';-('),\n",
       " (7, ':-|'),\n",
       " (6, 'X,'),\n",
       " (6, ':-o'),\n",
       " (6, ':-\\\\'),\n",
       " (6, ':-*'),\n",
       " (6, ':$'),\n",
       " (5, 'XL'),\n",
       " (5, ':d'),\n",
       " (5, ':X'),\n",
       " (5, ':H'),\n",
       " (5, ':?'),\n",
       " (5, ':-S'),\n",
       " (4, ';-D'),\n",
       " (3, ':Z'),\n",
       " (3, ':E'),\n",
       " (3, ':-s'),\n",
       " (3, ':-['),\n",
       " (3, ':-X'),\n",
       " (2, 'X5'),\n",
       " (2, 'X-('),\n",
       " (2, \"X's\"),\n",
       " (2, ';-;'),\n",
       " (2, ':}'),\n",
       " (2, ':D'),\n",
       " (2, ':;'),\n",
       " (2, \":'D\"),\n",
       " (1, 'x|'),\n",
       " (1, \"x'd\"),\n",
       " (1, \"x'D\"),\n",
       " (1, ';-|'),\n",
       " (1, ';-/'),\n",
       " (1, ':-x'),\n",
       " (1, ':-h'),\n",
       " (1, ':-]'),\n",
       " (1, ':-W'),\n",
       " (1, ':-$'),\n",
       " (1, ':('),\n",
       " (1, \":'[\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we are gonna find what emoticons are used in our dataset\n",
    "import re\n",
    "tweet_text=train_data.SentimentText.str.cat()\n",
    "emos=set(re.findall(r\" ([xX:;][-']?.) \",tweet_text))\n",
    "emos_count=[]\n",
    "\n",
    "for emo in emos:\n",
    "    emos_count.append((tweet_text.count(emo),emo))\n",
    "\n",
    "sorted(emos_count,reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should by now know which emoticons are used (and its frequency) to build two regex, one for the happy ones and another for the sad ones. We will then use them in the preprocessing process to mark them as using happy emoticons or sad ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy Emoticons: {';p', ';)', ';-D', ':-D', ';D', ':p', ';P', 'xD', ':D', ':d', ';d', 'x)', 'XD', 'xd', ';-)'}\n",
      "Sad emoticons: {':/', ':(', ':|', \":'(\"}\n"
     ]
    }
   ],
   "source": [
    "HAPPY_EMO=r\" ([xX;:]-?[dD)]|:-?[\\)]|[:;][pP]) \"\n",
    "SAD_EMO=r\" (:'?[/|\\(]) \"\n",
    "print(\"Happy Emoticons:\",set(re.findall(HAPPY_EMO,tweet_text)))\n",
    "print(\"Sad emoticons:\",set(re.findall(SAD_EMO,tweet_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOST USED WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are going to do next is to define a function that will show us top words, so we may fix things before running our learning algorithm. This function takes as input a text and output words sorted according to their frequency, starting with the most used word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\INDIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_used_words(text):\n",
    "    tokens=word_tokenize(text)\n",
    "    frequency_dist=nltk.FreqDist(tokens)\n",
    "    print(\"There is %d different words\" %len(str(tokens)))\n",
    "    \n",
    "    return sorted(frequency_dist,key=frequency_dist.__getitem__,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 13294450 different words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " '!',\n",
       " '.',\n",
       " 'I',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'you',\n",
       " '?',\n",
       " 'a',\n",
       " 'it',\n",
       " 'i',\n",
       " ';',\n",
       " 'and',\n",
       " '&',\n",
       " '...',\n",
       " 'my',\n",
       " 'for',\n",
       " 'is',\n",
       " 'that',\n",
       " \"'s\",\n",
       " \"n't\",\n",
       " 'in',\n",
       " 'me',\n",
       " 'of',\n",
       " 'have',\n",
       " 'on',\n",
       " 'quot',\n",
       " \"'m\",\n",
       " 'so',\n",
       " ':',\n",
       " 'but',\n",
       " '#',\n",
       " 'do',\n",
       " 'was',\n",
       " 'be',\n",
       " '..',\n",
       " 'not',\n",
       " 'your',\n",
       " 'are',\n",
       " 'just',\n",
       " 'with',\n",
       " 'like',\n",
       " '-',\n",
       " 'at',\n",
       " '*',\n",
       " 'too',\n",
       " 'get',\n",
       " 'good',\n",
       " 'u',\n",
       " 'up',\n",
       " 'know',\n",
       " 'all',\n",
       " 'this',\n",
       " 'now',\n",
       " 'no',\n",
       " 'we',\n",
       " 'out',\n",
       " ')',\n",
       " 'love',\n",
       " 'lol',\n",
       " 'can',\n",
       " 'what',\n",
       " 'one',\n",
       " '(',\n",
       " 'will',\n",
       " 'go',\n",
       " 'about',\n",
       " 'did',\n",
       " 'got',\n",
       " \"'ll\",\n",
       " 'there',\n",
       " 'amp',\n",
       " 'day',\n",
       " 'http',\n",
       " 'see',\n",
       " \"'re\",\n",
       " 'if',\n",
       " 'time',\n",
       " 'they',\n",
       " 'think',\n",
       " 'as',\n",
       " 'when',\n",
       " 'from',\n",
       " 'You',\n",
       " 'It',\n",
       " 'going',\n",
       " 'really',\n",
       " 'well',\n",
       " 'am',\n",
       " 'work',\n",
       " 'had',\n",
       " 'would',\n",
       " 'how',\n",
       " 'he',\n",
       " 'here',\n",
       " 'thanks',\n",
       " 'some',\n",
       " '....',\n",
       " 'haha']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_used_words(train_data.SentimentText.str.cat())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that stop words are the most used, but in fact they don't help us determine if a tweet is happy/sad, however, they are consuming memory and they are making the learning process slower, so we really need to get rid of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\INDIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 13294450 different words\n"
     ]
    }
   ],
   "source": [
    "mw=most_used_words(train_data.SentimentText.str.cat())\n",
    "most_words=[]\n",
    "for w in mw:\n",
    "    if len(most_words)==100:\n",
    "        break\n",
    "    if w in stopwords.words('english'):\n",
    "        continue\n",
    "    else:\n",
    "        most_words.append(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '&',\n",
       " \"'\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '....',\n",
       " '2',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " 'And',\n",
       " 'But',\n",
       " 'I',\n",
       " 'It',\n",
       " 'LOL',\n",
       " 'Oh',\n",
       " 'Thanks',\n",
       " 'That',\n",
       " 'The',\n",
       " 'You',\n",
       " 'amp',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'better',\n",
       " 'ca',\n",
       " 'come',\n",
       " 'could',\n",
       " 'day',\n",
       " 'dont',\n",
       " 'even',\n",
       " 'feel',\n",
       " 'fun',\n",
       " 'get',\n",
       " 'go',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'great',\n",
       " 'haha',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'http',\n",
       " 'im',\n",
       " 'know',\n",
       " 'last',\n",
       " 'like',\n",
       " 'lol',\n",
       " 'love',\n",
       " 'lt',\n",
       " 'make',\n",
       " 'miss',\n",
       " 'much',\n",
       " \"n't\",\n",
       " 'na',\n",
       " 'need',\n",
       " 'never',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'oh',\n",
       " 'one',\n",
       " 'quot',\n",
       " 'really',\n",
       " 'right',\n",
       " 'sad',\n",
       " 'say',\n",
       " 'see',\n",
       " 'sorry',\n",
       " 'still',\n",
       " 'sure',\n",
       " 'thanks',\n",
       " 'think',\n",
       " 'though',\n",
       " 'time',\n",
       " 'today',\n",
       " 'u',\n",
       " 'ur',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'way',\n",
       " 'well',\n",
       " 'wish',\n",
       " 'work',\n",
       " 'would',\n",
       " 'yeah',\n",
       " 'yes']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wgar we did is to filter only non stop words\n",
    "#We will now get a look to the top 1000 words\n",
    "sorted(most_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have noticed something, right? There are words that have the same meaning, but written in a different manner, sometimes in the plural and sometimes with a suffix (ing, es ...), this will make our model think that they are different words and also make our vocabulary bigger (waste of memory and time for the learning process). The solution is to reduce those words with the same root, this is called stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\INDIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I'm defining this function to use it in the\n",
    "#DAta Preparation phase\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokenize(text):\n",
    "    stemmer=SnowballStemmer('english')\n",
    "    stemmer=WordNetLemmatizer()\n",
    "    return [stemmer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "def lemmatize_tokenize(text):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this phase,we will transform our tweets into a more usable data \n",
    "#by our ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e are going to use the Bag of Words algorithm, which basically takes a text as input, extract words from it (this is our vocabulary) to use them in the vectorization process. When a tweet comes in, it will vectorize it by counting the number of occurrences of each word in our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we have this two tweets: \"I learned a lot today\" and \"hahaha I got you\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tweet / words\tI\tlearned\ta\tlot\ttoday\thahaha\tgot\tyou\n",
    "first\t1\t1\t1\t1\t1\t0\t0\t0\n",
    "second\t1\t0\t0\t0\t0\t1\t1\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract the words present in the two tweets, then for each tweet we count the occurrences of each word in our vocabulary.\n",
    "\n",
    "This is the simplest form of the Bag of Words algorithm, however, there is other variants, we are gonna use the TF-IDF (Term Frequency - Inverse Document Frequency) variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILDING THE PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good practice to make a pipeline of transformation for your data, it will make the process of data transformation really easy and reusable. We will implement a pipeline for transforming our tweets to something that our ML models can digest (vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to do some preprocessing of the tweerts\n",
    "#we will delete strings(like @,#,....) because we think that they will not help\n",
    "#in determing if the person is Happy/Sad\n",
    "\n",
    "class TextPreProc(BaseEstimator,TransformerMixin):\n",
    "    \n",
    "    def __init__(self,use_mention=False):\n",
    "        self.use_mention=use_mention\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        #we can choose between keeping the mentions or deleting them\n",
    "        if self.use_mention:\n",
    "            X=X.str.replace(r\"@[a-zA-Z0-9_]* \",\" @tags\")\n",
    "        else:\n",
    "            X=X.str.replace(r\"@[a-zA-Z0_9]* \",\"\")\n",
    "            \n",
    "        #Keeping only the word after the #\n",
    "        X=X.str.replace(\"#\",\"\")\n",
    "        X=X.str.replace(r\"[-\\.\\n]\", \"\")\n",
    "        #Removing HTML garbage\n",
    "        X=X.str.replace(r\"https?://\\S*\",\"\")\n",
    "        \n",
    "        #replace repeated letters with only two occurences\n",
    "        #heeeeeeeeelllllooooo =>  heelloo\n",
    "        X=X.str.replace(r\"(.)\\1+\",r\"\\1\\1\")\n",
    "        #mark emoticionsa as happy or sad\n",
    "        X=X.str.replace(HAPPY_EMO,\" happyemoticons \")\n",
    "        X=X.str.replace(SAD_EMO,\" sademoticons \")\n",
    "        X=X.str.lower()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the pipeline taht will ttransform our tweets to something eatable\n",
    "#You can see that we are using our previously defined stemmer, it will \n",
    "#take care of the stemming process\n",
    "#For stop words,we let the IDF do the job\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentiments=train_data['Sentiment']\n",
    "tweets=train_data['SentimentText']\n",
    "\n",
    "#I get those parameters from the  'Fine tune the model'  part\n",
    "vectorizer=TfidfVectorizer(tokenizer=lemmatize_tokenize,ngram_range=(1,2))\n",
    "\n",
    "pipeline=Pipeline([\n",
    "    ('text_pre_processing',TextPreProc(use_mention=True)),\n",
    "    ('vectorizer',vectorizer),\n",
    "])\n",
    "\n",
    "#Let's split our data into learning set and testing set\n",
    "#This process is done to test the efficiency of our model at the end\n",
    "#You shouldn't look at the test data after choosing the final model\n",
    "learn_data,test_data,sentiments_learning,sentiments_test=train_test_split(\n",
    "                            tweets,sentiments,test_size=0.3\n",
    ")\n",
    "\n",
    "#This will transform our learning data from simple text to vector\n",
    "#by going through the proprocessing transformer\n",
    "learning_data=pipeline.fit_transform(learn_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT a MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have our data ready to be processed by ML models, the question we should ask is which model to use?\n",
    "\n",
    "The answer varies depending on the problem and data, for example, it's known that Naive Bias has proven good efficacy against Text Based Problems.\n",
    "\n",
    "A good way to choose a model is to try different candidate, evaluate them using cross validation, then chose the best one which will be later tested against our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()\n",
    "bnb=BernoulliNB()\n",
    "mnb=MultinomialNB()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== logistic regression ======\n",
      "scores =  [0.80620531 0.81534366 0.81067375 0.80755722 0.81576734 0.80811042\n",
      " 0.81213592 0.81444673 0.82026538 0.81086301]\n",
      "mean =  0.812136873824804\n",
      "variance =  1.71004260460798e-05\n",
      "standard deviation =  0.004135266139691592\n",
      "score on learning data(accuracy) =  0.8718567836324151\n",
      "classification report = \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.85     28143\n",
      "           1       0.92      0.86      0.89     41849\n",
      "\n",
      "    accuracy                           0.87     69992\n",
      "   macro avg       0.87      0.87      0.87     69992\n",
      "weighted avg       0.88      0.87      0.87     69992\n",
      "\n",
      "\n",
      "===== bernoulliNB ======\n",
      "scores =  [0.78230337 0.79682947 0.78532927 0.78383459 0.79459081 0.78225047\n",
      " 0.7899474  0.79395572 0.79238674 0.78272067]\n",
      "mean =  0.788414851104457\n",
      "variance =  2.9604715168555785e-05\n",
      "standard deviation =  0.00544102151884697\n",
      "score on learning data(accuracy) =  0.9053606126414447\n",
      "classification report = \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.88     26238\n",
      "           1       0.97      0.88      0.92     43754\n",
      "\n",
      "    accuracy                           0.91     69992\n",
      "   macro avg       0.90      0.92      0.90     69992\n",
      "weighted avg       0.91      0.91      0.91     69992\n",
      "\n",
      "\n",
      "===== multinomialNB ======\n",
      "scores =  [0.80671892 0.80934809 0.80477465 0.8047603  0.8063761  0.80825161\n",
      " 0.80501175 0.80805369 0.8117924  0.80580033]\n",
      "mean =  0.8070887852784432\n",
      "variance =  4.690761528120875e-06\n",
      "standard deviation =  0.002165816596141251\n",
      "score on learning data(accuracy) =  0.8998028346096697\n",
      "classification report = \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87     25485\n",
      "           1       0.98      0.86      0.92     44507\n",
      "\n",
      "    accuracy                           0.90     69992\n",
      "   macro avg       0.89      0.91      0.90     69992\n",
      "weighted avg       0.91      0.90      0.90     69992\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models={\n",
    "    'logistic regression':lr,\n",
    "    'bernoulliNB':bnb,\n",
    "    'multinomialNB':mnb,\n",
    "}\n",
    "\n",
    "for model in models.keys():\n",
    "    scores=cross_val_score(models[model],learning_data,sentiments_learning,\n",
    "                          scoring='f1',cv=10)\n",
    "    print(\"=====\",model,\"======\")\n",
    "    print(\"scores = \",scores)\n",
    "    print(\"mean = \",scores.mean())\n",
    "    print(\"variance = \",scores.var())\n",
    "    print(\"standard deviation = \",scores.std())\n",
    "    \n",
    "    models[model].fit(learning_data,sentiments_learning)\n",
    "    \n",
    "    print(\"score on learning data(accuracy) = \",accuracy_score(\n",
    "        models[model].predict(learning_data),sentiments_learning))\n",
    "    print(\"classification report = \")\n",
    "    print(classification_report(models[model].predict(learning_data),sentiments_learning))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of those models is likely to be overfitting, I will choose the multinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use the GridSearchCV to choose the best parameters to use.\n",
    "\n",
    "What the GridSearchCV does is trying different set of parameters, and for each one, it runs a cross validation and estimate the score. At the end we can see what are the best parameter and use them to build a better classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('text_pre_processing', TextPreProc()),\n",
       "                                       ('vectorizer', TfidfVectorizer()),\n",
       "                                       ('model', MultinomialNB())]),\n",
       "             param_grid=[{'text_pre_processing__use_mention': [True, False],\n",
       "                          'vectorizer__max_features': [1000, 2000, 5000, 10000,\n",
       "                                                       20000, None],\n",
       "                          'vectorizer__ngram_range': [(1, 1), (1, 2)]}],\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_pipeline=Pipeline([\n",
    "    ('text_pre_processing',TextPreProc()),\n",
    "    ('vectorizer',TfidfVectorizer()),\n",
    "    ('model',MultinomialNB()),\n",
    "])\n",
    "\n",
    "params=[\n",
    "    {\n",
    "        'text_pre_processing__use_mention':[True,False],\n",
    "        'vectorizer__max_features':[1000,2000,5000,10000,20000,None],\n",
    "        'vectorizer__ngram_range':[(1,1),(1,2)],\n",
    "    },\n",
    "]\n",
    "\n",
    "grid_search=GridSearchCV(grid_search_pipeline,params,cv=5,scoring='f1')\n",
    "grid_search.fit(learn_data,sentiments_learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_pre_processing__use_mention': True,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our model against data other than the data used for training our model will show how well the model is generalising on new data.\n",
    "\n",
    "Note\n",
    "We shouldn't test to choose the model, this will only let us confirm that the choosen model is doing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(learning_data,sentiments_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data=pipeline.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7536086942027536"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(testing_data,sentiments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not badd but will try to improve it futher this result in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
